# -*- coding: utf-8 -*-
"""Streamlit testing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13DhnzFGLQc8j77aoC944KmenEuEAry-e

# **Non-parametric Tester**

*Auteur: Naomi Smulders, Online Dialogue, december 2021*

Notebook voor het runnen van non-parametrisch testen voor continue/discrete variabelen in een A/B-test.

Input: user level data met variantID en Test variabele (Order Value, Sessieduur, aantal ads, etc).


---
"""

!jupyter nbconvert --to script non_parametric.ipynd

"""## Import packages + define functions


"""

# Commented out IPython magic to ensure Python compatibility.
#Import packages
import numpy as np
import scipy as scp
import matplotlib.pyplot as plt
import seaborn as sea
import plotly.graph_objs as go
import pandas as pd
import plotly.express as px
# %matplotlib inline
import plotly.offline as py
import colorlover as cl
py.init_notebook_mode(connected=False)
path = '/Downloads'
import os
import base64
from scipy import stats
from scipy.stats import skewnorm
import random
from google.colab import files

## define functions
def raw_data_plotter (setA, setB, variable):
    fig, ax = plt.subplots(1, 1)
    ax.hist(setA, density=False, histtype='stepfilled', alpha=0.7, label = 'A', bins = 100)
    ax.hist(setB, density=False, histtype='stepfilled', alpha=0.7, label = 'B', bins = 100)
    ax.legend(loc='best', frameon=False)
    ax.set_title('Density curve')
    ax.set_xlabel(variable)
    ax.set_ylabel('Density')
    #plt.xlim([100:100])
    return plt.show()
def normality_check (sample, alpha, name):
    k2, p = stats.normaltest(sample)
    print("p = {:g}".format(p))
    if p < alpha:  # null hypothesis: x comes from a normal distribution
        print(name, " violates normality\n")
    else:
        print(name, "is from a normal distribution\n")

def SRM_check (sample_A, sample_B, alpha):
    #expected = (len(sample_A) + len(sample_B))/2
    observed = [len(sample_A), len(sample_B)]
    chi, p = stats.chisquare(observed)
    print ('Sample size A: '+ str(len(sample_A)), 'Sample size B: '+ str(len(sample_B)))
    print("p = {:g}".format(p))
    if p < alpha:  # null hypothesis: x comes from a normal distribution
        print("Sample Mismatch\n")
    else:
        print("Sample Equal (Gucci)\n")

def MWW_test(sampleA, sampleB, alpha):
    U, p =stats.mannwhitneyu(sampleA, sampleB)
    print('Statistics=%.3f, p=%.3f' % (U, p))
    if p < alpha:
        print("Difference is significant")
    else:
        print("No significant difference")

"""# Load data

"""

##load datafile
uploaded = files.upload()

pip install inquirer

## turn data into pandas DataFrame
import io
datafile = input ('Name of your datafile: ')
data = pd.read_csv(io.BytesIO(uploaded[datafile]))
print('Name of variables: ')
print(data.columns)
variantcolumn = input ('Whats the name of the column of your variant identifier?')
print('Names of variants:')
print(*data[variantcolumn].unique(),sep=', ')

"""# Run Mann-Whitney U"""

var1 = input('NUMBER_OF_SESSIONS or NUMBER_OF_PAGEVIEWS or NUMBER_OF_ARTICLE_PAGEVIEWS?')
varA = input('Name of variant A: ')
varB = input('Name of variant B: ')
if var1 not in data.columns:
    raise ValueError(f"Error: '{var1}' is not a valid column in your data!")

setA = data[var1][data[variantcolumn].astype(str) == varA].fillna(0)
setB = data[var1][data[variantcolumn].astype(str) == varB].fillna(0)
# Null en zero waardes loggen
nulls_setA = setA.isnull().sum()
zeros_setA = (setA == 0).sum()
nulls_setB = setB.isnull().sum()
zeros_setB = (setB == 0).sum()
print(f"Set A - NULLs: {nulls_setA}, Zeros: {zeros_setA}")
print(f"Set B - NULLs: {nulls_setB}, Zeros: {zeros_setB}")
# Zorg ervoor dat alle waarden numeriek zijn
setA = pd.to_numeric(setA, errors='coerce').fillna(0)
setB = pd.to_numeric(setB, errors='coerce').fillna(0)
# Plot de ruwe data
raw_data_plotter(setA, setB, str(var1))  # var1 wordt hier expliciet naar string geconverteerd
# Controleer normaliteit
print('Normality check')
normality_check(setA, 0.05, 'Set A')
normality_check(setB, 0.05, 'Set B')
# Controleer SRM
print('SRM check')
SRM_check(setA, setB, 0.05)
# Bereken gemiddelden en log de impact
print('Averages of datasets')
avgA = setA.mean()
avgB = setB.mean()
print('Average A: ' + str(round(avgA, 3)), 'Average B: ' + str(round(avgB, 3)))
percent_impact = ((avgB - avgA) / avgA) * 100 if avgA != 0 else float('inf')  # Voorkom deling door nul
print(f"Percentual impact: {round(percent_impact, 2)}%")
# Bereken medianen
medA = setA.median()
medB = setB.median()
print('Median A: ' + str(round(medA, 3)), 'Median B: ' + str(round(medB, 3)), '\n')
# Test op significantie
print('Test for significance')
MWW_test(setA, setB, 0.1)

"""# Run Mann-Whitney U (No zero values)"""

var1 = input('Copy your KPI from the index of the previous function without '' ')
varA = input('Name of variant A: ')
varB = input('Name of variant B: ')

if var1 not in data.columns:
    raise ValueError(f"Error: '{var1}' is not a valid column in your data!")

setA_raw = data[var1][data[variantcolumn].astype(str) == varA]
setB_raw = data[var1][data[variantcolumn].astype(str) == varB]

# Null en zero waardes loggen
nulls_setA = setA_raw.isnull().sum()
zeros_setA = (setA_raw.fillna(0) == 0).sum()
nulls_setB = setB_raw.isnull().sum()
zeros_setB = (setB_raw.fillna(0) == 0).sum()
print(f"Set A - NULLs: {nulls_setA}, Zeros: {zeros_setA}")
print(f"Set B - NULLs: {nulls_setB}, Zeros: {zeros_setB}")

# Zorg ervoor dat alle waarden numeriek zijn en geen null (na conversie!)
setA = pd.to_numeric(setA_raw, errors='coerce').fillna(0)
setB = pd.to_numeric(setB_raw, errors='coerce').fillna(0)

# Filter out zero values for analysis and plotting
setA_no_zeros = setA[setA != 0]
setB_no_zeros = setB[setB != 0]

# Plot de ruwe data zonder nullen en nullen
raw_data_plotter(setA_no_zeros, setB_no_zeros, str(var1))

# Controleer normaliteit op gefilterde data
print('Normality check')
normality_check(setA_no_zeros, 0.05, 'Set A (no zeros)')
normality_check(setB_no_zeros, 0.05, 'Set B (no zeros)')

# Controleer SRM op gefilterde data
print('SRM check')
SRM_check(setA_no_zeros, setB_no_zeros, 0.05)

# Bereken gemiddelden en log de impact op gefilterde data
print('Averages of datasets (no zeros)')
avgA = setA_no_zeros.mean()
avgB = setB_no_zeros.mean()
print('Average A: ' + str(round(avgA, 3)), 'Average B: ' + str(round(avgB, 3)))
percent_impact = ((avgB - avgA) / avgA) * 100 if avgA != 0 else float('inf')
print(f"Percentual impact: {round(percent_impact, 2)}%")

# Bereken medianen
medA = setA_no_zeros.median()
medB = setB_no_zeros.median()
print('Median A: ' + str(round(medA, 3)), 'Median B: ' + str(round(medB, 3)), '\n')

# Test op significantie op gefilterde data
print('Test for significance')
MWW_test(setA_no_zeros, setB_no_zeros, 0.1)

"""# Run Mann-Whitney U - trimmed for outliers

"""

# Functie om Winsorizing toe te passen
def apply_winsorizing(data, lower_percentile=0.05, upper_percentile=0.95):
    lower_bound = data.quantile(lower_percentile)
    upper_bound = data.quantile(upper_percentile)
    winsorized_data = data.clip(lower=lower_bound, upper=upper_bound)

    # Log de grenzen en het aantal aangepaste waarden
    print(f"Winsorizing applied: lower bound = {lower_bound}, upper bound = {upper_bound}")
    adjusted_count = ((data < lower_bound).sum() + (data > upper_bound).sum())
    print(f"Number of adjusted values: {adjusted_count}")
    return winsorized_data

# Start van de code
var1 = input('NUMBER_OF_SESSIONS or NUMBER_OF_PAGEVIEWS or NUMBER_OF_ARTICLE_PAGEVIEWS?')
varA = input('Name of variant A: ')
varB = input('Name of variant B: ')

if var1 not in data.columns:
    raise ValueError(f"Error: '{var1}' is not a valid column in your data!")

# Selecteer de data voor de varianten
setA = data[var1][data[variantcolumn].astype(str) == varA].fillna(0)
setB = data[var1][data[variantcolumn].astype(str) == varB].fillna(0)

# Null en zero waardes loggen
nulls_setA = setA.isnull().sum()
zeros_setA = (setA == 0).sum()
nulls_setB = setB.isnull().sum()
zeros_setB = (setB == 0).sum()
print(f"Set A - NULLs: {nulls_setA}, Zeros: {zeros_setA}")
print(f"Set B - NULLs: {nulls_setB}, Zeros: {zeros_setB}")

# Zorg ervoor dat alle waarden numeriek zijn
setA = pd.to_numeric(setA, errors='coerce').fillna(0)
setB = pd.to_numeric(setB, errors='coerce').fillna(0)

# Winsorizing toepassen
print("Applying Winsorizing to datasets...")
setA_winsorized = apply_winsorizing(setA, lower_percentile=0.05, upper_percentile=0.95)
setB_winsorized = apply_winsorizing(setB, lower_percentile=0.05, upper_percentile=0.95)

# Nu pas starten met de berekeningen na Winsorizing

# Plot de ruwe data
raw_data_plotter(setA_winsorized, setB_winsorized, str(var1))

# Controleer normaliteit
print('Normality check')
normality_check(setA_winsorized, 0.05, 'Set A')
normality_check(setB_winsorized, 0.05, 'Set B')

# Controleer SRM
print('SRM check')
SRM_check(setA_winsorized, setB_winsorized, 0.05)

# Bereken gemiddelden en log de impact
print('Averages of datasets')
avgA = setA_winsorized.mean()
avgB = setB_winsorized.mean()
print('Average A: ' + str(round(avgA, 3)), 'Average B: ' + str(round(avgB, 3)))
percent_impact = ((avgB - avgA) / avgA) * 100 if avgA != 0 else float('inf')  # Voorkom deling door nul
print(f"Percentual impact: {round(percent_impact, 2)}%")

# Bereken medianen
medA = setA_winsorized.median()
medB = setB_winsorized.median()
print('Median A: ' + str(round(medA, 3)), 'Median B: ' + str(round(medB, 3)), '\n')

# Test op significantie
print('Test for significance')
MWW_test(setA_winsorized, setB_winsorized, 0.1)